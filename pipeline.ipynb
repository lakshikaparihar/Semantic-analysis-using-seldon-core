{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "43caff5dcc899c0940b8095911d08397191658f39841b9bb6363f0ab677136ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import unicodedata\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordcounts(self,x):\n",
    "\t    self.length = len(str(x).split())\n",
    "\t    return self.length\n",
    "\n",
    "    def get_charcounts(self,x):\n",
    "    \tself.s = x.split()\n",
    "    \tx = ''.join(self.s)\n",
    "    \treturn len(x)\n",
    "\n",
    "    def get_avg_wordlength(self,x):\n",
    "    \tself.count = get_charcounts(x)/get_wordcounts(x)\n",
    "    \treturn self.count\n",
    "\n",
    "    def _get_stopwords_counts(self,x):\n",
    "    \tself.l = len([t for t in x.split() if t in stopwords])\n",
    "    \treturn self.l\n",
    "\n",
    "    def _get_hashtag_counts(self,x):\n",
    "    \tself.l = len([t for t in x.split() if t.startswith('#')])\n",
    "    \treturn self.l\n",
    "\n",
    "    def _get_mentions_counts(self,x):\n",
    "    \tself.l = len([t for t in x.split() if t.startswith('@')])\n",
    "    \treturn self.l\n",
    "\n",
    "    def _get_digit_counts(self,x):\n",
    "    \tself.digits = re.findall(r'[0-9,.]+', x)\n",
    "    \treturn len(self.digits)\n",
    "\n",
    "    def _get_uppercase_counts(self,x):\n",
    "    \treturn len([t for t in x.split() if t.isupper()])\n",
    "\n",
    "    def _cont_exp(self,x):\n",
    "    \tabbreviations = json.load(open(\"abbereviations_wordlist.json\"))\n",
    "\n",
    "    \tif type(x) is str:\n",
    "    \t\tfor key in abbreviations:\n",
    "    \t\t\tself.value = abbreviations[key]\n",
    "    \t\t\tself.raw_text = r'\\b' + key + r'\\b'\n",
    "    \t\t\tx = re.sub(self.raw_text, self.value, x)\n",
    "    \t\t\t# print(raw_text,value, x)\n",
    "    \t\treturn x\n",
    "    \telse:\n",
    "    \t\treturn x\n",
    "\n",
    "\n",
    "    def get_emails(self,x):\n",
    "    \tself.emails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', x)\n",
    "    \tself.counts = len(self.emails)\n",
    "\n",
    "    \treturn self.counts, self.emails\n",
    "\n",
    "\n",
    "    def remove_emails(self,x):\n",
    "    \treturn re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n",
    "\n",
    "    def get_urls(self,x):\n",
    "    \tself.urls = re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)\n",
    "    \tself.counts = len(self.urls)\n",
    "\n",
    "    \treturn self.counts, self.urls\n",
    "\n",
    "    def remove_urls(self,x):\n",
    "    \treturn re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n",
    "\n",
    "    def remove_rt(self,x):\n",
    "    \treturn re.sub(r'\\brt\\b', '', x).strip()\n",
    "\n",
    "    def remove_special_chars(self,x):\n",
    "    \tx = re.sub(r'[^\\w ]+', \"\", x)\n",
    "    \tx = ' '.join(x.split())\n",
    "    \treturn x\n",
    "\n",
    "    def remove_html_tags(self,x):\n",
    "    \treturn BeautifulSoup(x, 'html.parser').get_text().strip()\n",
    "\n",
    "    def remove_accented_chars(self,x):\n",
    "    \tx = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \treturn x\n",
    "\n",
    "    def remove_stopwords(self,x):\n",
    "    \treturn ' '.join([t for t in x.split() if t not in stopwords])\t\n",
    "\n",
    "    def make_base(self,x):\n",
    "    \tx = str(x)\n",
    "    \tself.x_list = []\n",
    "    \tself.doc = nlp(x)\n",
    "    \n",
    "    \tfor token in doc:\n",
    "    \t\tself.lemma = token.lemma_\n",
    "    \t\tif self.lemma == '-PRON-' or self.lemma == 'be':\n",
    "    \t\t\tself.lemma = token.text\n",
    "\n",
    "    \t\tself.x_list.append(self.lemma)\n",
    "    \treturn ' '.join(self.x_list)\n",
    "\n",
    "    def get_value_counts(self,df, col):\n",
    "    \tself.text = ' '.join(df[col])\n",
    "    \tself.text = self.text.split()\n",
    "    \tself.freq = pd.Series(self.text).value_counts()\n",
    "    \treturn self.freq\n",
    "\n",
    "    def remove_common_words(self,x, freq, n=20):\n",
    "    \tself.fn = freq[:n]\n",
    "    \tx = ' '.join([t for t in x.split() if t not in self.fn])\n",
    "    \treturn x\n",
    "\n",
    "    def remove_rarewords(self,x, freq, n=20):\n",
    "    \tself.fn = freq.tail(n)\n",
    "    \tx = ' '.join([t for t in x.split() if t not in self.fn])\n",
    "    \treturn x\n",
    "\n",
    "    def remove_dups_char(self,x):\n",
    "    \tx = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    \treturn x\n",
    "\n",
    "    def spelling_correction(self,x):\n",
    "    \tx = TextBlob(x).correct()\n",
    "    \treturn x\n",
    "\n",
    "    def get_basic_features(self,df):\n",
    "    \tif type(df) == pd.core.frame.DataFrame:\n",
    "    \t\tdf['char_counts'] = df['text'].apply(lambda x: get_charcounts(x))\n",
    "    \t\tdf['word_counts'] = df['text'].apply(lambda x: get_wordcounts(x))\n",
    "    \t\tdf['avg_wordlength'] = df['text'].apply(lambda x: get_avg_wordlength(x))\n",
    "    \t\tdf['stopwords_counts'] = df['text'].apply(lambda x: _get_stopwords_counts(x))\n",
    "    \t\tdf['hashtag_counts'] = df['text'].apply(lambda x: _get_hashtag_counts(x))\n",
    "    \t\tdf['mentions_counts'] = df['text'].apply(lambda x: _get_mentions_counts(x))\n",
    "    \t\tdf['digits_counts'] = df['text'].apply(lambda x: _get_digit_counts(x))\n",
    "    \t\tdf['uppercase_counts'] = df['text'].apply(lambda x: _get_uppercase_counts(x))\n",
    "    \telse:\n",
    "    \t\tprint('ERROR: This function takes only Pandas DataFrame')\n",
    "    \n",
    "    \treturn df\n",
    "\n",
    "\n",
    "    def get_ngram(self,df, col, ngram_range):\n",
    "    \tself.vectorizer = CountVectorizer(ngram_range=(ngram_range, ngram_range))\n",
    "    \tself.vectorizer.fit_transform(df[col])\n",
    "    \tself.ngram = self.vectorizer.vocabulary_\n",
    "    \tself.ngram = sorted(ngram.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    \treturn self.ngram\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x):\n",
    "        x = str(x).lower().replace('\\\\','').replace('_',' ')\n",
    "        x = self._cont_exp(x)\n",
    "        x = self.remove_emails(x)\n",
    "        x = self.remove_urls(x)\n",
    "        x = self.remove_html_tags(x)\n",
    "        x = self.remove_rt(x)\n",
    "        x = self.remove_accented_chars(x)\n",
    "        x = self.remove_special_chars(x)\n",
    "        x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC  #we will use svm model\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"IMDB-Movie-Reviews-Large-Dataset-50k/train.xlsx\")\n"
   ]
  },
  {
   "source": [
    "df[\"Reviews\"]=df[\"Reviews\"].apply(lambda x:TextNormalizer().transform(x))"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n['When I first tuned in on this morning news, I thought, \"wow, finally, some entertainment.\" It was slightly amusing for a week or so... But we have to face it, these news reporters (if one can even call them that) have WAY TOO MUCH \"playing around\" time.<br /><br />At first, I thought Jillian was a breathe of fresh air. But seriously, this woman has got not the least bit of journalist in her. She is very unprofessional. She keeps on interrupting Steve when he starts informing the viewers about a certain news report. It\\'s just really become annoying to the point that I can\\'t watch it anymore.<br /><br />Jillian is NOT a good journalist. Hell, she\\'s more of a celebrity who loves being a celebrity. Hence, she instantly transforms into a celebrity around celebrities whom she\\'s supposed to be interviewing. She\\'s not very professional and quite possibly perceives her relationship with celebrities more important than being a rightfully insatiable journalist- and that\\'s all I can say about her.<br /><br />Also (disappointingly), this show has more entertainment news than necessary news reports about the world, the government, the US, or something that will benefit and/or serve the public\\'s best interest. They\\'re too focus on sensationalism that everything they talk about comes off as a commercial product. On the other hand, their field reporters are interestingly tolerable...<br /><br />I believe \"Good Day LA\" is for young teenagers and celebrities, and it is definitely not for people who actually CARE about the news.<br /><br />SIDE NOTE: (I\\'d really rather watch KTLA. However, they try so hard to be entertaining sometimes. They\\'re still a bit dull though. Oh well, I\\'ll stick to NBC\\'s \"Today.\" ABC\\'s \"Good Morning America\" is also okay... as long as Diane Sawyer doesn\\'t become way too serious.)'\n 'Mere thoughts of \"Going Overboard\" (aka \"Babes Ahoy\") make me want to weep. Throwing yourself out a window would be better than watching this movie. It\\'s not even a supposed \"so bad it\\'s good\" movie. I would spend money to buy copies of this movie and burn them so that people can\\'t see it. Oh the pain, the pain...'\n 'Why does this movie fall WELL below standards? Ultimately, the answer lies in the poor, humourless script. A slim/average looking Travolta (looking rather dapper in black I must say, even with a HUGE mullet) and Gross both act very well as two young-ish \\'slick-dressed\\' but nevertheless dimwitted New Yorkers eager to open their own nightclub. Other than that, the rest of the film is just boring to watch. It is SO dull that it\\'s really not worth knowing what happens in the film\\'s climax on any level. Kelly Preston obviously exudes sex appeal and the sexually charged dance with her husband-to-be Travolta is one of the film\\'s few pleasures. Charles Martin Smith is quite fun to watch as struggling KGB honcho \"Bob Smith\". Personally, I think the movie would have been better if the plot was altered a little so that the settings did not change from NY to \\'Indian Springs, Nebraska\\' (which is in the former Soviet Union?)--you\\'ll understand if you see the movie... Apparently, this movie was filmed in 1986 ready for a 1987 release. I guess Paramount stalled on releasing the movie until January 1989 because of the unbelievable plot. It was reported they deemed it \"unreleasable\". Nevertheless, this $6,000,000 film garnered a little over an embarrassing $163,000 in revenue as it was released only BRIEFLY in places like Texas and Colorado before heading straight-to-video. This is testament to the overall BAD quality of this movie.'\n ...\n 'My God. This movie was awful. I can\\'t complain about it too much. I went to see it just to be grossed out. It did suffice, sort of. It\\'s funny that the most disgusting part of the movie was in the very, very beginning where the woman is extremely vividly forced to give birth to a horribly mutated baby.<br /><br />I also think that it\\'s funny that the most notable actor in the movie was the Hispanic soldier, who was a supporting actor in Next Friday. Everyone in the movie did a horrible acting job. It was some of the worst acting I\\'ve ever paid to see. <br /><br />I also expected that it would be much more gruesome than the first one. It wasn\\'t. I expected it to be more gruesome because it\\'s a sequel and horror movie sequels are usually much less successful than their predecessors. I expected it to be more gruesome since gore and violence usually sell a horror movie these days (Grudge 2, Saw 3, Jeepers Creepers 1 & 2, Dead Silence), but It actually wasn\\'t nearly as gruesome as the first one, which was yet another disappointment. <br /><br />The mutants in the first one were kind of disturbing but the filmmakers were trying so hard in this one to make them creepy that they were absolutely hilarious.<br /><br />I also hated the entire concept of showing the clip of the female soldier\\'s son on her camera-phone saying \"I love you, mommy\" FOUR TIMES. It was stupid to show it in the first place because they were just trying to make us feel worse for the vulnerable mother than the rest of the soldiers, and it was even more stupid to keep trying to make us feel even WORSE for her by showing it three more times for no reason. This movie was a joke.'\n 'When I first popped in Happy Birthday to Me, I checked the timer to see how long the film was. I was amazed at the length. Both animated and horror films share a common ground: attention span of the selected audience and that should be at or right around 90 minutes. Anything more, and you\\'ll lose the bulk of your audience.<br /><br />This 110 minutes, or 20 minutes past its prime was a huge problem for me. I\\'d like to say half of this movie could\\'ve been edited out, but I would be too generous to say that. Go ahead and watch it and tell me how many scenes could\\'ve been edited, even without being a film major.<br /><br />Regardless of the overstayed visit, the movie was below mediocre. It spent all of its time trying to be this huge mystery on which of the \"elite 10\" is killing off the remaining friends. For the most part, they not only over-do it, but they zoom in on a face and pretty much say \"It\\'s this guy! No! It\\'s this gal!\" You\\'ll spend more time with the camera misleading you than actually enjoying the movie. And don\\'t get me started on the acting.<br /><br />Okay, that got me started. I had to laugh in the beginning trying to remember if Melissa Sue Anderson played the character that went blind on Little House on the Prairie (later, research proved my suspicions correct) because all the way through this movie, she genuinely looked blind. Strange, as an established actress, she should\\'ve been the best of the group, but turned out the worst. The rest of the staff, aside from Ann (Bregman) was pretty damn bad, too, but she, uh, took the cake.<br /><br />The movie begins with a group of ten friends, and one\\'s immediately killed off. Barely anyone thinks twice of this \"dear\" friend\\'s disappearance, so they continue on their merry way. Slowly, then more rapidly, there are revelations about Virginia\\'s (Anderson), the main character, past and her psychologist, who\\'s a tad bit more personal (AND ON CALL 24/7, apparently) than most shrinks. All the while, more and more deaths occur.<br /><br />What\\'s funny is, just as the first \"disappearance,\" the more \"best buds\" vanish, the less the rest care. Sure, they give a few seconds of air time to say \"Wow, (that person) just wouldn\\'t run off\" etc, but then they\\'re back to their sexual ways. And speaking of which, it\\'s probably due to the horrid script, or maybe it was I who was losing interest at minute 30, but it was really hard to keep up with who liked who of the group as they all seemed to be sexual partners of the next or someone would either be freaked out to the MAX by another and best friends the next scene. SEE: the creepy guy that kept a mouse/rat in his pocket \\x96 literally \\x96 and was the most obvious suspect. I\\'m giving the film too much credit (and time,) but how he became part of the \"elite 10\" I\\'ll never know.<br /><br />But, I digress, there\\'s a mystery here. Why are these kids targets? Why is Virginia thinking she\\'s killed someone, when it was never proved (\\'till the end) that any of them actually has been slaughtered? And why would the trailer and poster claim these killings to be \"Six of the most bizarre murders you will ever see\"? Hell, even for 1981, most of these had been shown in any of the first two Friday the 13th films \\x96 coincidentally enough, Friday the 13th Part 2 was released 2 weeks to the day of Happy Birthday to Me. Perhaps, they\\'re speaking of when they filmed it months prior, but were late to the, well, party.<br /><br />When the \"secrets\" are revealed, trust me, you\\'ll have to rewind 3-4x to actually get the laughable and incoherent motives, and even then, put the subtitles on to get all the mumbling victim/killer\\'s words. Even if you get the first time, it\\'s an unbelievably outrageous and hilarious finale. It\\'s almost worth watching the whole movie again, but as a drinking game.<br /><br />This birthday gathering should be avoided. It\\'s a horrible and illogical first draft script \\x96 please, please know it takes multiple rewrites before the cameras role, it contains either way under acting or extreme over acting and it\\'s 100% unrealistic on how people react in extraordinary circumstances.<br /><br />Side Note: When I was a kid, or say 10-11 years old, I loved horror films. (Still do, oddly. Definite guilty pleasures, but they are getting harder and harder to watch as years pass.) We got our first VCR, and I taped as many horror films off network (or, EDITED VERSIONS) TV. All I remember of Happy Birthday to Me is getting the last 10 minutes on tape, which scared me to death \\x96 and obviously gave away the big mystery on who the killer was. Even though I have seen other clips of this movie, I think this is the first full-length viewing I\\'ve had. Thankfully, this awful movie didn\\'t wound me as a child. I am older now, and I can take this trash. But never again.<br /><br />Side Note 2: That said, that crazy \"Happy Birthday to Me\" song played in the end credits (and as a score throughout) still creeps me out tremendously. I guess, this movie (or last few minutes,) did have an influence on my childhood. Shame on you, Melissa Sue Anderson!'\n 'So why does this show suck? Unfortunately, that really is the only question, because there is no doubt that it does.<br /><br />For those unfamiliar with the premise of the show, the doomed-to-be-shortlived series Cavemen focuses on a number of Neanderthals and their struggle to exist in modern day America and is based on the characters featured in a series of television ads for Geico Insurance. The concept is solid and there is every reason to think it could be executed successfully.<br /><br />I had to think about it for awhile, but then the tagline from the commercials -- something to the effect of \"We\\'re not that much different from you\" provided me with the key to the show\\'s suckiness. Even though cavemen/Neanderthals are actually a different species than humanity, the title characters of this show, it turns out, are exactly the same as those of us who are boring jerks.<br /><br />Maybe its my background as a game writer -- rather than a soulless, hack, committee-based writer from California -- but this show had so much potential, and none of it has been realized. To start with, the producers should have focused on the fun things that would make cavemen different from us.<br /><br />What could conceivably be funny, for example, about giving them occupations like perpetual grad student and furniture store clerk, when they would have more compellingly been drawn to things like subterranean utility workers and guides at cave parks? Why would they play prosaic games like squash, when a whole episode could be devoted to them trying get hunting licenses to go after game with spears? A show like this could write itself, and it takes some willfully bad writing to make it quite so crappy and boring.<br /><br />Another tiresome aspect of this show is an attempt to portray the cavemen as being subjected to a number of stereotypes associated with various human minorities. Yawn! This has been done so many times before, and never more drearily than this. And, as noted previously, Neanderthals really are a different species, so using them as a metaphor for racial stereotyping is both uncompelling and off the mark.<br /><br />Responses are welcome, including those from anyone who wants to tell me why I\\'m wrong. I\\'d like to enjoy this show and am just sorry that I have thus far been unable to.<br /><br />Michael J. Varhola, Skirmisher Online Gaming Magazine']\n"
     ]
    }
   ],
   "source": [
    "X = df[\"Reviews\"].values\n",
    "y = df[\"Sentiment\"].values\n",
    "print(type(X))\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(X):\n",
    "    X = pd.Series(X)\n",
    "    print(\"Entering..................\")\n",
    "    X=X.apply(lambda x:TextNormalizer().transform(x))\n",
    "    print(\"Wowwwww...................\")\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "func = FunctionTransformer(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " x_train,x_test, y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer',TextNormalizer()),\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('trainer',LinearSVC())\n",
    "])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n(25000,)\n<class 'numpy.ndarray'>\n(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X.shape)\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-4719cf73997a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m                 **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    308\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m             raise ValueError(\n\u001b[1;32m-> 1194\u001b[1;33m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m                 \"string object received.\")\n\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "source": [
    "print(model.predict(x_test))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entering..................\n",
      "Wowwwww...................\n",
      "(5000,)\n",
      "['pos' 'neg' 'neg' ... 'pos' 'neg' 'pos']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entering..................\nWowwwww...................\n(1,)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['pos'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "sample = \"I Love the movie , SRK's acting was great\"\n",
    "model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entering..................\nWowwwww...................\n(1,)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "sample = \"Salman Khan's acting was worst\"\n",
    "model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model,\"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}