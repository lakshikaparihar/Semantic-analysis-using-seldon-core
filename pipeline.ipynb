{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "43caff5dcc899c0940b8095911d08397191658f39841b9bb6363f0ab677136ac"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC  #we will use svm model\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "import unicodedata\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords  = set(nltk.corpus.stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordcounts(self,x):\n",
    "\t    self.length = len(str(x).split())\n",
    "\t    return self.length\n",
    "\n",
    "    def get_charcounts(self,x):\n",
    "    \tself.s = x.split()\n",
    "    \tx = ''.join(self.s)\n",
    "    \treturn len(x)\n",
    "\n",
    "    def get_avg_wordlength(self,x):\n",
    "    \tself.count = get_charcounts(x)/get_wordcounts(x)\n",
    "    \treturn self.count\n",
    "\n",
    "    def _get_stopwords_counts(self,x):\n",
    "    \tself.l = len([t for t in x.split() if t in stopwords])\n",
    "    \treturn self.l\n",
    "\n",
    "    def _get_hashtag_counts(self,x):\n",
    "    \tself.l = len([t for t in x.split() if t.startswith('#')])\n",
    "    \treturn self.l\n",
    "\n",
    "    def _get_mentions_counts(self,x):\n",
    "    \tself.l = len([t for t in x.split() if t.startswith('@')])\n",
    "    \treturn self.l\n",
    "\n",
    "    def _get_digit_counts(self,x):\n",
    "    \tself.digits = re.findall(r'[0-9,.]+', x)\n",
    "    \treturn len(self.digits)\n",
    "\n",
    "    def _get_uppercase_counts(self,x):\n",
    "    \treturn len([t for t in x.split() if t.isupper()])\n",
    "\n",
    "    def _cont_exp(self,x):\n",
    "    \tabbreviations = json.load(open(\"abbereviations_wordlist.json\"))\n",
    "\n",
    "    \tif type(x) is str:\n",
    "    \t\tfor key in abbreviations:\n",
    "    \t\t\tself.value = abbreviations[key]\n",
    "    \t\t\tself.raw_text = r'\\b' + key + r'\\b'\n",
    "    \t\t\tx = re.sub(self.raw_text, self.value, x)\n",
    "    \t\t\t# print(raw_text,value, x)\n",
    "    \t\treturn x\n",
    "    \telse:\n",
    "    \t\treturn x\n",
    "\n",
    "\n",
    "    def get_emails(self,x):\n",
    "    \tself.emails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', x)\n",
    "    \tself.counts = len(self.emails)\n",
    "\n",
    "    \treturn self.counts, self.emails\n",
    "\n",
    "\n",
    "    def remove_emails(self,x):\n",
    "    \treturn re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)\n",
    "\n",
    "    def get_urls(self,x):\n",
    "    \tself.urls = re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)\n",
    "    \tself.counts = len(self.urls)\n",
    "\n",
    "    \treturn self.counts, self.urls\n",
    "\n",
    "    def remove_urls(self,x):\n",
    "    \treturn re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)\n",
    "\n",
    "    def remove_rt(self,x):\n",
    "    \treturn re.sub(r'\\brt\\b', '', x).strip()\n",
    "\n",
    "    def remove_special_chars(self,x):\n",
    "    \tx = re.sub(r'[^\\w ]+', \"\", x)\n",
    "    \tx = ' '.join(x.split())\n",
    "    \treturn x\n",
    "\n",
    "    def remove_html_tags(self,x):\n",
    "    \treturn BeautifulSoup(x, 'html.parser').get_text().strip()\n",
    "\n",
    "    def remove_accented_chars(self,x):\n",
    "    \tx = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \treturn x\n",
    "\n",
    "    def remove_stopwords(self,x):\n",
    "    \treturn ' '.join([t for t in x.split() if t not in stopwords])\t\n",
    "\n",
    "    def make_base(self,x):\n",
    "    \tx = str(x)\n",
    "    \tself.x_list = []\n",
    "    \tself.doc = nlp(x)\n",
    "    \n",
    "    \tfor token in doc:\n",
    "    \t\tself.lemma = token.lemma_\n",
    "    \t\tif self.lemma == '-PRON-' or self.lemma == 'be':\n",
    "    \t\t\tself.lemma = token.text\n",
    "\n",
    "    \t\tself.x_list.append(self.lemma)\n",
    "    \treturn ' '.join(self.x_list)\n",
    "\n",
    "    def get_value_counts(self,df, col):\n",
    "    \tself.text = ' '.join(df[col])\n",
    "    \tself.text = self.text.split()\n",
    "    \tself.freq = pd.Series(self.text).value_counts()\n",
    "    \treturn self.freq\n",
    "\n",
    "    def remove_common_words(self,x, freq, n=20):\n",
    "    \tself.fn = freq[:n]\n",
    "    \tx = ' '.join([t for t in x.split() if t not in self.fn])\n",
    "    \treturn x\n",
    "\n",
    "    def remove_rarewords(self,x, freq, n=20):\n",
    "    \tself.fn = freq.tail(n)\n",
    "    \tx = ' '.join([t for t in x.split() if t not in self.fn])\n",
    "    \treturn x\n",
    "\n",
    "    def remove_dups_char(self,x):\n",
    "    \tx = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "    \treturn x\n",
    "\n",
    "    def spelling_correction(self,x):\n",
    "    \tx = TextBlob(x).correct()\n",
    "    \treturn x\n",
    "\n",
    "    def get_basic_features(self,df):\n",
    "    \tif type(df) == pd.core.frame.DataFrame:\n",
    "    \t\tdf['char_counts'] = df['text'].apply(lambda x: get_charcounts(x))\n",
    "    \t\tdf['word_counts'] = df['text'].apply(lambda x: get_wordcounts(x))\n",
    "    \t\tdf['avg_wordlength'] = df['text'].apply(lambda x: get_avg_wordlength(x))\n",
    "    \t\tdf['stopwords_counts'] = df['text'].apply(lambda x: _get_stopwords_counts(x))\n",
    "    \t\tdf['hashtag_counts'] = df['text'].apply(lambda x: _get_hashtag_counts(x))\n",
    "    \t\tdf['mentions_counts'] = df['text'].apply(lambda x: _get_mentions_counts(x))\n",
    "    \t\tdf['digits_counts'] = df['text'].apply(lambda x: _get_digit_counts(x))\n",
    "    \t\tdf['uppercase_counts'] = df['text'].apply(lambda x: _get_uppercase_counts(x))\n",
    "    \telse:\n",
    "    \t\tprint('ERROR: This function takes only Pandas DataFrame')\n",
    "    \n",
    "    \treturn df\n",
    "\n",
    "\n",
    "    def get_ngram(self,df, col, ngram_range):\n",
    "    \tself.vectorizer = CountVectorizer(ngram_range=(ngram_range, ngram_range))\n",
    "    \tself.vectorizer.fit_transform(df[col])\n",
    "    \tself.ngram = self.vectorizer.vocabulary_\n",
    "    \tself.ngram = sorted(ngram.items(), key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    \treturn self.ngram\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,x):\n",
    "        x = str(x).lower().replace('\\\\','').replace('_',' ')\n",
    "        x = self._cont_exp(x)\n",
    "        x = self.remove_emails(x)\n",
    "        x = self.remove_urls(x)\n",
    "        x = self.remove_html_tags(x)\n",
    "        x = self.remove_rt(x)\n",
    "        x = self.remove_accented_chars(x)\n",
    "        x = self.remove_special_chars(x)\n",
    "        x = re.sub(\"(.)\\\\1{2,}\", \"\\\\1\", x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"IMDB-Movie-Reviews-Large-Dataset-50k/train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"Reviews\"].values\n",
    "y = df[\"Sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(X):\n",
    "    X = pd.Series(X)\n",
    "    print(\"Starting.....................................................................\")\n",
    "    X=X.apply(lambda x:TextNormalizer().transform(x))\n",
    "    print(\"Data Cleaning...........................................................\")\n",
    "    return X\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "func = FunctionTransformer(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " x_train,x_test, y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.2)"
   ]
  },
  {
   "source": [
    "model = Pipeline([\n",
    "    ('vectorizer',func),\n",
    "    ('tfidf',TfidfVectorizer()),\n",
    "    ('trainer',LinearSVC())\n",
    "])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting.....................................................................\n",
      "Data Cleaning...........................................................\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 FunctionTransformer(func=<function transformer at 0x00000177E0766D38>)),\n",
       "                ('tfidf', TfidfVectorizer()), ('trainer', LinearSVC())])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "source": [
    "print(model.predict(x_test))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting.....................................................................\n",
      "Data Cleaning...........................................................\n",
      "['pos' 'neg' 'neg' ... 'pos' 'neg' 'pos']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting.....................................................................\nData Cleaning...........................................................\n['pos']\n"
     ]
    }
   ],
   "source": [
    "sample = \"I Love the movie , SRK's acting was great\"\n",
    "print(model.predict(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting.....................................................................\nData Cleaning...........................................................\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['neg'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "sample = \"Salman Khan's acting was worst\"\n",
    "model.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model,\"model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entering..................\nWowwwww...................\n(1,)\n['pos']\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(\"model.joblib\")\n",
    "sample = \"I Love the movie , SRK's acting was great\"\n",
    "print(model.predict(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}